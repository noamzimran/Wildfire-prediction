{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from pprint import pprint\n",
    "import folium\n",
    "from folium import plugins\n",
    "from folium.plugins import HeatMap\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import pygeohash as pgh\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from osgeo import gdal\n",
    "from shapely.geometry import Point, LineString, Polygon, MultiPoint\n",
    "from shapely.ops import nearest_points\n",
    "import geopandas as gpd\n",
    "import gdal\n",
    "import fiona\n",
    "\n",
    "import requests\n",
    "import urllib\n",
    "from time import sleep\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Organizing data\n",
    "\n",
    "1. Connecting to sql wildfire database\n",
    "2. creating a pandas df\n",
    "3. Reducing the dataset:\n",
    "    - Finding the number of wildfires in every state\n",
    "    - Choosing one state for the case study\n",
    "\n",
    "\n",
    "* One-time execution of that part,\n",
    " no need to run again after getting CSV file of the organized data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data sources\n",
    "Wildfires - https://www.kaggle.com/rtatman/188-million-us-wildfires\n",
    "Weather - https://www.kaggle.com/selfishgene/historical-hourly-weather-data\n",
    "DEM - SRTM"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Connect to Wildfire database file\n",
    "con = sqlite3.connect(r'data\\FPA_FOD_20170508.sqlite')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read to DataFrame\n",
    "df = pd.read_sql_query('SELECT * FROM Fires;',con)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Heat map according to FIRE_SIZE\n",
    "map_ = folium.Map(location=[df[\"LATITUDE\"].mean(), df[\"LONGITUDE\"].mean()],\n",
    "                    tiles = \"Stamen Terrain\",\n",
    "                    zoom_start = 3)\n",
    "\n",
    "\n",
    "heat_df = df[[\"LATITUDE\",\"LONGITUDE\", \"FIRE_YEAR\", \"FIRE_SIZE\"]]\n",
    "# limit number of points, to get a processible result\n",
    "heat_df = heat_df[heat_df[\"FIRE_SIZE\"] > 1800]\n",
    "heat_df = heat_df.dropna(axis=0, subset=[\"LATITUDE\",\"LONGITUDE\"])\n",
    "\n",
    "# List comprehension to make out list of lists\n",
    "heat_data = [[row[\"LATITUDE\"],row[\"LONGITUDE\"]] for index, row in heat_df.iterrows()]\n",
    "del heat_df\n",
    "\n",
    "# Plot it on the map\n",
    "HeatMap(heat_data, min_opacity=.4, max_val=.8).add_to(map_)\n",
    "\n",
    "# Display the map\n",
    "map_\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Reduce Database"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check the distribution of fires between the states\n",
    "fire_state = df.pivot_table(index=['STATE'],aggfunc='size')\n",
    "print(fire_state)\n",
    "plt.figure(figsize=(20, 10))\n",
    "fire_state.plot.bar(rot=70 ,align='center', width=0.3 ,title=\"Number of fire in every state\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We decided to focus on California which has the maximum number of fires"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "CA_df = df.loc[df['STATE'] == 'CA']\n",
    "CA_df.to_csv(r'results\\California_fires.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# take only 25% of CA fires randomly\n",
    "# CA_df_small = CA_df.sample(frac = 0.25)\n",
    "# CA_df_small.to_csv(r'data\\California_fires_25.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Close sql connection\n",
    "con.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Case study California wildfires"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# read CSV\n",
    "# CA_fires = pd.read_csv(r'data\\California_fires_25.csv')\n",
    "CA_fires = pd.read_csv(r'results\\California_fires.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create GeoDataFrame\n",
    "locations = gpd.points_from_xy(x=CA_fires.LONGITUDE, y=CA_fires.LATITUDE)\n",
    "CA_fires['geometry'] = locations\n",
    "CA_fires.head()\n",
    "geo_fires = gpd.GeoDataFrame(CA_fires, geometry='geometry', crs=\"EPSG:4326\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "California Heat map"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "map_CA = folium.Map(location=[geo_fires[\"LATITUDE\"].mean(), geo_fires[\"LONGITUDE\"].mean()],\n",
    "                    tiles = \"Stamen Terrain\",\n",
    "                    zoom_start = 3)\n",
    "\n",
    "\n",
    "heat_df2 = geo_fires[[\"LATITUDE\",\"LONGITUDE\", \"FIRE_YEAR\", \"FIRE_SIZE\"]]\n",
    "# limit number of points, to get a processible result\n",
    "heat_df2 = heat_df2[heat_df2[\"FIRE_SIZE\"] > 1800]\n",
    "heat_df2 = heat_df2.dropna(axis=0, subset=[\"LATITUDE\",\"LONGITUDE\"])\n",
    "\n",
    "# List comprehension to make out list of lists\n",
    "heat_data2 = [[row[\"LATITUDE\"],row[\"LONGITUDE\"]] for index, row in heat_df2.iterrows()]\n",
    "del heat_df2\n",
    "\n",
    "# Plot it on the map\n",
    "HeatMap(heat_data2, min_opacity=.4, max_val=.8).add_to(map_CA)\n",
    "\n",
    "# Display the map\n",
    "map_CA"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The wildfires dataset cover the years 1992-2015,\n",
    "while the weather dataset begins in Oct' 2012 and ends in 2017.\n",
    "So we got an overlap between Oct' 2012 to the end of 2015.\n",
    "\n",
    "Also we decided to reduce the prediction area between LAT 32 to 35."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "geo_fires_small = geo_fires.loc[(geo_fires['LATITUDE'] >= 32) & (geo_fires['LATITUDE'] <= 35)\n",
    "                                & (geo_fires['FIRE_YEAR'] >= 2012) & (geo_fires['FIRE_YEAR'] <= 2015)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save to shapefile\n",
    "geo_fires_small.to_file(r'results\\shp\\fires_2012-15.shp')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create cells\n",
    "We created the cells in QGIS by splitting SRTM tiles to 10X10 cells and converting to polygons.\n",
    "Every tile is 1$^\\circ$ X 1$^\\circ$ which is about 100X100 km,\n",
    "so every cell is about 10X10 km."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load cells polygons\n",
    "geo_cells = gpd.read_file(r'data\\cells_polygons_california_only.shp',crs=\"EPSG:4326\")\n",
    "geo_cells.head()\n",
    "\n",
    "geo_cells = geo_cells.drop(columns=['path','layer'])\n",
    "geo_cells.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load fires data\n",
    "fires = gpd.read_file(r'results\\shp\\fires_2012-15.shp',crs=\"EPSG:4326\")\n",
    "fires.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# calculate topographic statistics for each cell\n",
    "for i, cell in geo_cells.iterrows():\n",
    "    path = str(cell['location'][str.find(cell['location'],'data'):])\n",
    "    raster = gdal.Open(path)\n",
    "    band = raster.GetRasterBand(1)\n",
    "    stats = band.ComputeStatistics(0)\n",
    "    geo_cells.at[i,'min_height'] = stats[0]\n",
    "    geo_cells.at[i,'max_height'] = stats[1]\n",
    "    geo_cells.at[i,'avg_height'] = stats[2]\n",
    "    geo_cells.at[i,'std_height'] = stats[3]\n",
    "\n",
    "geo_cells.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save\n",
    "geo_cells.to_file(r'results\\shp\\geo_cells_basic.shp',crs=\"EPSG:4326\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Manege weather data\n",
    "\n",
    "We got an hourly weather datasets of about 30 cities around the world.\n",
    "The relevant data is temperature, humidity, pressure and wind speed.\n",
    "We found 3 cities that covers our study case area - Los Angeles, San Diego and Las Vegas.\n",
    "\n",
    "first we are going to change the dataset to weekly basis (because we want to make weekly predictions),\n",
    "then we are going to find the closest city for every cell to get the relevant values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def weeklyWeather(file_path):\n",
    "    \"\"\"\n",
    "    Get hourly weather data and return weekly data\n",
    "    :param file_path: CSV file path\n",
    "    :return: DataFrame of weekly weather in Los Angeles, San Diego and Las Vegas\n",
    "    \"\"\"\n",
    "\n",
    "    # read csv data\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df[['datetime','Los Angeles', 'San Diego', 'Las Vegas']]\n",
    "    # convert to date feature\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    # extract years months and weeks from the date features\n",
    "    df['year']= df['datetime'].dt.year\n",
    "    df['month']= df['datetime'].dt.month\n",
    "    df['week_of_year'] = df['datetime'].dt.week\n",
    "    # take data only between years 2012-2015\n",
    "    df = df.loc[(df['year'] >= 2012) & (df['year'] <= 2015)]\n",
    "    # solve a specific case when the last days of one year counts in the 'week_of_year' of the next year\n",
    "    df.loc[(df['month'] == 12) & (df['week_of_year'] == 1) , 'year'] += 1\n",
    "    df.drop(columns=['month'], inplace=True )\n",
    "    # calculate the average weather value of every week\n",
    "    return df.groupby(['year','week_of_year'],as_index=False).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temperature = weeklyWeather(r'data\\weather\\temperature.csv')\n",
    "humidity = weeklyWeather(r'data\\weather\\humidity.csv')\n",
    "pressure = weeklyWeather(r'data\\weather\\pressure.csv')\n",
    "wind_speed = weeklyWeather(r'data\\weather\\wind_speed.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save CSV\n",
    "temperature.to_csv(r'results\\weekly_weather\\temperature_weekly.csv')\n",
    "humidity.to_csv(r'results\\weekly_weather\\humidity_weekly.csv')\n",
    "pressure.to_csv(r'results\\weekly_weather\\pressure_weekly.csv')\n",
    "wind_speed.to_csv(r'results\\weekly_weather\\wind_speed_weekly.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# read weather\n",
    "temperature = pd.read_csv(r'results\\weekly_weather\\temperature_weekly.csv')\n",
    "humidity = pd.read_csv(r'results\\weekly_weather\\humidity_weekly.csv')\n",
    "pressure = pd.read_csv(r'results\\weekly_weather\\pressure_weekly.csv')\n",
    "wind_speed= pd.read_csv(r'results\\weekly_weather\\wind_speed_weekly.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Find the nearest city to every cell"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def nearest(row, gdf1, gdf2, geom_col1,geom_col2, src_column=None):\n",
    "    \"\"\"\n",
    "    Find the nearest point and return the corresponding value from specified column\n",
    "    :param row: row of gdf1\n",
    "    :param gdf1: GeoDataFrame of points that we want to find their nearest point from gdf2\n",
    "    :param gdf2: GeoDataFrame of points that we want to find the nearest point for them from gdf1\n",
    "    :param geom_col1: Name of geometry column in df1\n",
    "    :param geom_col2: Name of geometry column in df2\n",
    "    :param src_column: Name of the column that contain the values\n",
    "    :return: The values of the nearest points in new column of gdf1\n",
    "    \"\"\"\n",
    "    geom_union = gdf2.geometry.unary_union\n",
    "    # Find the geometry that is closest\n",
    "    nearest_ = gdf2[geom_col2] == nearest_points(row[geom_col1], geom_union)[1]\n",
    "    # Get the corresponding value from df2 (matching is based on the geometry)\n",
    "    value = gdf2.loc[nearest_,src_column].to_numpy()[0]\n",
    "    return value"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# read cities weather\n",
    "df2 = pd.read_csv(r'data\\cities_weather.csv')\n",
    "locations_points = gpd.points_from_xy(x=df2.lon, y=df2.lat)\n",
    "df2['geometry'] = locations_points\n",
    "\n",
    "cities = gpd.GeoDataFrame(df2, geometry='geometry', crs=\"EPSG:4326\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# read cells\n",
    "geo_cells = gpd.read_file(r'results\\shp\\geo_cells_basic.shp',crs=\"EPSG:4326\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# find centroid of every cell's polygon\n",
    "# projection to epsg=3310\n",
    "proj_cells = geo_cells.to_crs(epsg=3310)\n",
    "# calc centroids\n",
    "proj_cells['centroid'] = proj_cells.centroid\n",
    "# projection back to epsg=4326 (geographic)\n",
    "centroid_4326 = proj_cells.centroid.to_crs(epsg=4326)\n",
    "geo_cells['centroid'] = centroid_4326\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# find cells's nearest city\n",
    "geo_cells['nearest city'] = geo_cells.apply(nearest, gdf1=geo_cells,\n",
    "                                                        gdf2=cities, geom_col1='centroid',geom_col2='geometry' , src_column='city', axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check the number of closed points fot every city\n",
    "closed_cities = geo_cells.pivot_table(index=['nearest city'], aggfunc='size')\n",
    "closed_cities"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# saving to shapefile\n",
    "# remove the centroid column to stay with only one geometric column\n",
    "geo_cells.drop('centroid', axis=1, inplace=True)\n",
    "# save\n",
    "geo_cells.to_file(r'results\\shp\\geo_cells_with_cities.shp')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# create cell for every week in every year"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# read\n",
    "fires = gpd.read_file(r'results\\shp\\fires_2012-15.shp',crs=\"EPSG:4326\")\n",
    "geo_cells = gpd.read_file(r'results\\shp\\geo_cells_with_cities.shp')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "geo_cells_with_weeks = geo_cells.copy()\n",
    "geo_cells_with_weeks['week'] = 39\n",
    "geo_cells_with_weeks['year'] = 2012"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for 2012\n",
    "\n",
    "for i in range(40,52):\n",
    "    gd = geo_cells.copy()\n",
    "    gd['week'] = i\n",
    "    gd['year'] = 2012\n",
    "    geo_cells_with_weeks = geo_cells_with_weeks.append(gd)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for 2013-2015\n",
    "\n",
    "for j in range (2013,2016):\n",
    "    for i in range(0,52):\n",
    "        gd = geo_cells.copy()\n",
    "        gd['week'] = i\n",
    "        gd['year'] = j\n",
    "        geo_cells_with_weeks = geo_cells_with_weeks.append(gd)\n",
    "\n",
    "geo_cells_with_weeks.reset_index(drop=True, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# fitting fires to cells in year 2012\n",
    "for i in range(39,52):\n",
    "    temp_fires = fires[fires['FIRE_YEAR']==2012]\n",
    "    temp_fires = temp_fires[fires['DISCOVER_1']//7==i]\n",
    "    for j, row in geo_cells.iterrows():\n",
    "        pip = temp_fires.within((geo_cells.loc[j,'geometry'])) # check if inside cell\n",
    "        fire_in_cell = temp_fires[pip==True]\n",
    "        if fire_in_cell.shape[0] >0 : # case of fire in cell\n",
    "            index = j + (i-39)*geo_cells.shape[0] # finding the index of the right cell\n",
    "            geo_cells_with_weeks.at[index,'fire'] = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# fitting fires to cells in years 2013-2015\n",
    "for y in range(2013,2016):\n",
    "    for i in range(52):\n",
    "        temp_fires = fires[fires['FIRE_YEAR']==y]\n",
    "        temp_fires = temp_fires[fires['DISCOVER_1']//7==i]\n",
    "        for j, row in geo_cells.iterrows():\n",
    "            pip = temp_fires.within((geo_cells.loc[j,'geometry'])) # check if inside cell\n",
    "            fire_in_cell = temp_fires[pip==True]\n",
    "            if fire_in_cell.shape[0] >0 : # case of fire in cell\n",
    "                index = j + i*geo_cells.shape[0] + 13*geo_cells.shape[0] + ((y-2013)*52*geo_cells.shape[0]) # finding the index of the right cell\n",
    "                geo_cells_with_weeks.at[index,'fire'] = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#save to shp\n",
    "geo_cells_with_weeks.to_file(r'results\\shp\\geo_cells_with_weeksANDcities.shp')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Insert weather to database"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# read\n",
    "geo_cells_with_weeks = gpd.read_file(r'results\\shp\\geo_cells_with_weeksANDcities.shp')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def insert_weather_data(cells_df, weather_df):\n",
    "    weather_name =[x for x in globals() if globals()[x] is weather_df][0]\n",
    "    for i, row in weather_df.iterrows():\n",
    "        for j in range(0,1040):\n",
    "            city = cells_df.loc[j,'nearest ci']\n",
    "            index = j + 1040*i\n",
    "            if index < cells_df.shape[0]: # because the weeks's count of the weather different from the cell counts\n",
    "                cells_df.loc[index,weather_name] = weather_df.loc[i,city]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "insert_weather_data(geo_cells_with_weeks,temperature)\n",
    "insert_weather_data(geo_cells_with_weeks,humidity)\n",
    "insert_weather_data(geo_cells_with_weeks,pressure)\n",
    "insert_weather_data(geo_cells_with_weeks,wind_speed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# take only the relevant rows in case of extra rows\n",
    "cells_with_weather = geo_cells_with_weeks.loc[0:175759,:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "#save to shp\n",
    "cells_with_weather.to_file(r'results\\shp\\cells_with_weather.shp')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# read\n",
    "cells_finish = gpd.read_file(r'results\\shp\\cells_with_weather.shp')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "for index, row in cells_finish.iterrows():\n",
    "    cells_finish.at[index,'ULcorner_Lat'] = row['geometry'].boundary.xy[1][0]\n",
    "    cells_finish.at[index,'ULcorner_Long'] = row['geometry'].boundary.xy[0][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "cells_finish.to_file(r'results\\shp\\cells_finish.shp')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}